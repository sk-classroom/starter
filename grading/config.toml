# LLM Quiz Challenge Configuration File
# This file contains default parameters and settings for the quiz challenge system

[api]
# API endpoint configuration
base_url = "https://openrouter.ai/api/v1"

[models]
# Model configuration - choose models that support structured outputs
quiz_model = "openrouter/google/gemma-3-4b-it"        # Model for answering questions
evaluator_model = "openrouter/google/gemini-2.5-flash-lite"        # Model for evaluating answers (needs structured output support)

[parameters]
# LLM parameters
context_window_size = 32768       # Context window size for models
max_tokens = 1500                  # Maximum tokens in responses (increased from default 500)

[context]
# Context materials - URLs to fetch for providing context to the quiz model
urls = [
    "https://raw.githubusercontent.com/skojaku/adv-net-sci/refs/heads/main/docs/lecture-note/m01-euler_tour/01-concepts.qmd",
    "https://raw.githubusercontent.com/skojaku/adv-net-sci/refs/heads/main/docs/lecture-note/m01-euler_tour/04-advanced.qmd"
]

[output]
# Output configuration
#results_file = "./assignment/quiz_results.json"  # Default output file for results
verbose = false                     # Enable verbose logging

# Example usage:
# python llm_quiz_grading.py --quiz-file quiz.toml --api-key sk-xxx --config config.toml
#
# Configuration file values can be overridden by command line arguments:
# python llm_quiz_grading.py --quiz-file quiz.toml --api-key sk-xxx --config config.toml --max-tokens 1000
